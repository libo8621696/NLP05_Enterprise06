### 简答题： 

##### 1、ERNIE 1.0 (20'), XLNET(20'), RoBERTa(20'), ALBERT(20') 分别基于 BERT 做了哪些改进？

- 对于ERNIE1.0而言，其主要贡献是其主要的贡献在于通过实体和短语mask学习到了语法和句法信息，并且将中文维基百科、百度百科、百度新闻以及百度贴吧加入了训练数据集，的特点是同时实现了单字级别、实体级别以及短语级别的MASK。阶段主要是通过提供更多的语料以及实体级别的连续MASK整合进了大量的先验知识，从而提升了预测语言的表示效果，其在自然语言推理、语义相似性、命名实体识别NER、情感分析、问答抽取等任务方面进行了相关的测试，在准确度上要强于BERT超过1%.
- XLNET是一个类BERT模型，高度融合了以GPT为代表的自回归模型和以BERT为代表的自编码模型二者的优点，属于双向语言模型，并且可以根据上下文单词预测被MASK掉的噪音，完成生成类NLP任务。为了更好的利用好AR模型的优点并且双向捕捉到上下文信息，XLNet采用了Permutation Language Modeling这种基于排列组合的输入方法，不会改变原始词的顺序，实现通过Attention的MASK来对应的不同分解方法。为了学习更多有用的表示方式，将目标的位置变化重新参数化，在目标位置Z_T 上从前文获取注意力，采用了两套表征方式CONTENT STREAM或者QUERY STREAM进行双流自注意力的表征。训练方法是采用50%的概率是连续的句子，50%的概率是不连续的句子，随机采样排列组合数据，与BERT比较不同的地方在于XL-NET采用了相对片段编码（RELATIVE SEGMENT ENCODINGS）。XLNET在包括问答、自然语言推理、情感分析以及文档排序等超过20项任务上大幅超过了BERT性能。
- RoBERTa是facebook提出的BERT优化版本，是A Robustly Optimizer BERT Pretraining Approach的简称，是保持结构不变，整体改动并不大。通过让模型接受更多的数据来进行预测效果的提升，其主要集中在更长的训练时间、更大的batch size，以及更多的训练数据进行着手。在训练方法上同样去掉了NSP预测任务，并且进行动态MASK，引入了新型的文本编码方式。NSP的局限性体现在输入空间的局限性，由于句子长短不一，容易出现补padding的时候造成预测的准确性下降。BERT级别的文本编码是字符级别的文本编码BPE词典达到30K，而RoBERTa是基于byte(字节)级别的进行文本编码从而增加数据量到50K。在SQuAD 1.1/2.0、MNLL-m、以及SST-2等数据集上RoBERTa的得分强于BERT超过3%。
- ALBERT主要集中在进行轻量级的BERT设计。该方法是通过矩阵分解和参数共享减少参数，通过SOP替换NSP，并且采用n-gram MASK进行改进。这种矩阵分解的思想是通过因子嵌入参数化来实现，在两个大维度之间加入一个小维度起到降维的作用。SOP的本质是逆序，通过补偿一部分因为嵌入和FFN共享所损失的性能，将负样本换成了同一篇文章中的两个逆序的句子，SOP主要是进行主题预测和通顺度预测。其中的N-GRAM MASK是预测包含更完整语义信息的n-gram片段。AIBERT将窄而深的模型转变成了宽而浅的模型。通过以上方法，ALBERT相比于BERT减少了内存消耗、提升了训练速度、相比于BERT-large使用更少的模型参数达到了更好的结果。

##### 2、ALBERT为什么用 SOP 任务替代BERT 中的 NSP 任务？(20')
- SOP又称为Sentence Order Prediction的简称，即句子顺序预测。ALBERT通过SOP任务替代NSP任务的主要原因是NSP会给整体的BERT运行框架性能带来不利影响。NSP任务主要考虑的是关于话题迁移的建模方法，NSP的损失函数对于SOP任务而言影响很小，然而SOP的损失函数对于NSP而言影响较大。主要原因在于SOP补偿了一部分因为 embedding 和 FFN 共享而损失的性能；同时，NSP将”topic prediction”和“coherence prediction”融合了起来造成学习比较困难。相比而言。SOP将负样本换成了同一篇文章中的两个逆序的句子，进而消除“topic prediction”。